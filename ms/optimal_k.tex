\documentclass[a4paper,11pt]{article}

\usepackage{geometry}
\geometry{a4paper,margin=1in}

\usepackage{url}
\usepackage{color}
\usepackage{subfig}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\newcommand{\Lemma}[1]{Lemma~\ref{#1}}
\newcommand{\kristoffer}[1]{{\color{red}{#1}}}
\newcommand{\alex}[1]{{\color{blue}{#1}}}

\newcommand{\DB}{\mathsf{DB}_{k,a}}
\newcommand{\U}{\mathsf{U}_{k,a}}
\newcommand{\ST}{\mathsf{ST}_{k,a}}
\newcommand{\UN}{\mathsf{UN}_{k,a}}
\newcommand{\dplus}{\delta^+_{k,a}}
\newcommand{\dminus}{\delta^-_{k,a}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\abu}{\alpha}
\newcommand{\esize}{{\sf E_{size}}}
\newcommand{\st}{\:|\:}
\renewcommand{\geq}{\geqslant}
\renewcommand{\ge}{\geqslant}
\renewcommand{\leq}{\leqslant}
\renewcommand{\le}{\leqslant}


%%% BEGIN DOCUMENT
\begin{document}

\title{Optimal\_k - DB-graph inference by accurate sampling \\Fast and accurate selection of parameters for genome assembly (by sampling)} 
\author{}
\date{} % delete this line to display the current date
\maketitle

\section{Abstract}
Motivation: There is no clear way on how to chose parameters k-mer size and abundance for a De Bruijn based de novo assembler. As \emph{de novo} genome assembly is time consuming for large genomes, it is of importance to chose these parameters well in order to prevent multiple runs. Current software for estimating $k$ only optimize certain features such as maximizing the number of genomic k-mers. There is a need for more clear objectives such as E-size or N50.

Results:
We provide a method (optimal\_k) to estimate average unitig length, N50 and E-size for all combinations of minimum abundance and $k$ in one run. As unitigs are a foundation of the de Bruijn graph, estimating these quantities provides an understanding of the quality of a DBG based genome assembly as well as a good base for chosing the best combination of $k$ and abundance. The estimations obtained by optimal\_k are extremely accurate. [We also note that these estimations also accurately predict the best quality for DBG based assemblers that perform more steps such as tip removals, bubble popping and usage of paried end read information. ]

\section{Introduction} % (fold)
\label{sec:introduction}

\kristoffer{ Mention that there are not many tools for computing optimal parameters at all. And make sure to mention that memry is not the issue. Mention the positives about our methods like speed and clear objective function but make sure to mention that it's memory requiring but thats not a problem if you are going to do the assembly anyway!!}
% section introduction (end)

A unitig of a graph is a maximal unary path. In the contig assembly phase, popular genome assemblers report a unitig decomposition of the assembly graph, after some artifacts have been been dealt with, like tip removal and bubble popping.

\section{Methods} % (fold)
\label{sec:methods}

The general idea is to provide the user with metrics such as unitigs N50 and E-Size and average number of genomic vertices in a DBG  for all possible k-mer sizes and abundances. \kristoffer{ We implement a FM-index data structure described in cite XX. This allows us to query a k-mer, its in and out neighbors in O() time. }  We furthermore derive formulas for how much we need to sample in order to reach a given accuracy on all our estimates. 

\noindent \textbf{Notation.} We assume that the input consists of a set $R$ of $n$ reads or length $r$. We denote by $\K_k$ be the multiset of all $k$-mers in the reads; observe that $|\K_k| = n(r-k+1)$. Moreover, we denote by $\DB$ the de Bruijn graph of \emph{order} $k$ and \emph{minimum abundance} $a$ built on $R$. That is, the set of vertices of $\DB$ is the set of all $k$-mers in the reads which occur at least $a$ times in $R$, and two vertices of $\DB$ are connected by an arc if they have a suffix-prefix overlap of length $k-1$. Let $V(\DB)$ denote the set of vertices of $\DB$. For all $v \in V(\DB)$, let $\abu(v)$ denote the abundance of the $k$-mer $v$. We denote by $\dplus(v)$ the number of out-neighbors of $v$ in $\DB$, and by $\dminus(v)$ the number of out-neighbors of $v$ in $\DB$. These values can be obtained by queries to the index built on the set $R$. 

A node $v$ of $\DB$ is called \emph{unary} if $\dminus(v) = \dplus(v) = 1$, and it is called \emph{isolated} if $\dminus(v) = \dplus(v) = 0$. A path in $\DB$ is called a \emph{unitig} if all its internal vertices are unary, and its two extremities are not. When clear from the context, we will also use the term unitig to denote the string spelled by a unitig path in $\DB$. 
%Given a unitig $w = (v_1,v_2,\dots,v_t)$ of $\DB$, we denote by $|w|_n$ the number of nodes of $w$, i.e., $|w|_n = t$, and by $|w|_s$ the length of the string spelled by $w$, that is, $|w|_s = k + t - 1$.

\alex{Talk about reverse complements in dB graphs: a k-mer and its reverse complement are bundled into the same node and the abundances are added up. Say how we deal with this case in practice.}

\alex{Give pseudo-code of how we get the in/out degrees for all abundances.}

\alex{Say that one of the main ideas is to do weighted sampling.}

\subsection{Sampling algorithms} % (fold)
\label{sub:algorithm}

%\subparagraph{Estimating the number of nodes of a DBG} % (fold)
%\label{subp:estimating_the_number_of_nodes_in_a_db_graph}

\noindent \textbf{Estimating the number of nodes of a dBG.} Let $V(\DB)$ denote the set of vertices of $\DB$, and let $\mathbb{I}(x,a)$ be an indicator variable returning $1$ if the $k$-mer $x$ has abundance at least $a$ in $R$, and $0$ otherwise. We can write

\[|V(\DB)| = \sum_{x \in \K_k} \frac{1}{\abu(x)}\mathbb{I}(x,a).\]
Since $V(\DB)$ is a subset of the multiset $\K_k$, we can consider the proportion 

\[p_{k,a} := \frac{|V(\DB)|}{|\K_k|} = \frac{\sum_{x \in \K_k} \frac{1}{\abu(x)}\mathbb{I}(x,a)}{|\K_k|} \in [0,1].\]
%
%
%The multiset of $k$-mers from the reads that are vertices of $\DB$ and its complement partitions the multiset $\K_k$. The (multi)set of k-mers that are members of $X$ and it's complement partitions $\K$. The true proportion $p_k$ of $X$ in $\K_k$ is given by
%\begin{equation}
%	p_k = \frac{\sum_{k\in \K_k} \frac{1}{a_k}I_{k\geq a} }{ \sum_{k\in \K_k} }
%\end{equation}
We can estimate $p_{k,a}$ by sampling a multiset $\{x_1,\dots,x_m\}$ of $k$-mers from $\K_k$, and taking
\[\hat{p}_{k,a} := \frac{\sum_{i = 1}^m \frac{1}{\abu(x_i)}\mathbb{I}(x_i, a)}{m}.\]

Therefore, we also get an estimate of $X := |V(\DB)|$ as $\hat{X} = \hat{p}_{k,a}|\K_k| = \hat{p}_{k,a} n(r-k+1)$. By the observations in Sec.~\ref{sec:sampling-accuracy}, we immediately get how many samples $m$ we need in order to bound the relative error of $\hat{X}$ within a certain confidence interval.

\alex{Say that we can implement this for all abundances}

\medskip
\noindent \textbf{Estimating the number of unitigs of a dBG.} Let $\U$ denote the set of all unitigs of $\DB$. We derive now a simple combinatorial expression for $|\U|$, which is key in the sampling phase. Let $\ST$ denote the set of start nodes of the unitigs of $\DB$, that is, 
\begin{align*}
\ST := \{v \in V(\DB) \:&|\: \dplus(v) \geq 2 \text{ or } \\
& (\dplus(v) = 1 \text{ and } \dminus(v) \neq 1) \text{ or} \\
& (\dplus(v) = 0 \text{ and } \dminus(v) = 0)\}.
\end{align*}
Since every node $v$ in $\ST$ is either an isolated node, or it is a start node of a different unitig, starting with $v$ and then continuing to each of its out-neighbors, we can write
\[|\U| = \sum_{v \in \ST} \max(1,\dplus(v)).\]

As above, we can also obtain this number also by summing over all $k$-mers in the reads:

\[|\U| = \sum_{x \in \K_k} \max\left(\frac{1}{\abu(x)}\mathbb{I}(x,a),\frac{1}{\abu(x)}\mathbb{I}(x,a)\dplus(x)\right).\]
% subparagraph estimating_the_number_of_nodes_in_a_db_graph (end)

We consider the ratio between the number of unitigs and all $k$-mers in the reads
\[q_{k,a} := \frac{|\U|}{|\K_k|} = \frac{\sum_{x \in \K_k} \max\left(\frac{1}{\abu(x)}\mathbb{I}(x,a),\frac{1}{\abu(x)}\mathbb{I}(x,a)\dplus(x)\right)}{|\K_k|}.\]
Observe that $q_{k,a} \in [0,1]$ since every unitig contains at least one $k$-mer, thus $|\U| \leq |\K_k|$. We can analogously estimate it, after sampling a multiset $\{x_1,\dots,x_m\}$ of $k$-mers from $\K_k$, as
\[\hat{q}_{k,a} := \frac{\sum_{i = 1}^{m} \max\left(\frac{1}{\abu(x_i)}\mathbb{I}(x,a),\frac{1}{\abu(x_i)}\mathbb{I}(x_i,a)\dplus(x_i)\right)}{m}.\]

The estimate of $Y := |\U|$ is then $\hat{Y} = \hat{q}_{k,a}|\K_k| = \hat{q}_{k,a}n(r-k+1)$. By the observations in Sec.~\ref{sec:sampling-accuracy}, we immediately get how many samples $m$ we need in order to bound the relative error of $\hat{Y}$ within a certain confidence interval.
\alex{say we can implement this for all abundances.}

\medskip
\noindent\textbf{Estimating the average length of the unitigs of a dBG.} We are now interested in determining the average length of the strings spelled by the unitigs of $\DB$. 

Denote by the \emph{truncated length} of a unitig $w = (v_1,v_2,\dots,v_t)$ the number of its internal vertices plus its start vertex. We first estimate the average truncated lengths of the unitigs of $\DB$, and then obtain the average unitig string length by summing $k$.\footnote{This assumes, in order to simplify the presentation, that the dBG has no isolated nodes, which are unitigs with $0$ internal nodes and truncated length $1$, but spell strings of length $k$. However, isolated nodes can be easily accounted for as separate case in all the formulas.} Working with the truncated unitig lengths guarantees that we can again easily bound the sampling error.

Let $\UN$ denote the set of unary nodes of $\DB$; notice that $\UN \cap \ST = \emptyset$. The average truncated length of the unitigs is obtained as 
\begin{equation}
\frac{|\UN \cup \ST|}{|\ST|}.
\label{eqn:avg_internal}
\end{equation}

As above, we can write
\[|\UN| = \sum_{\begin{subarray}{c} x \in \K_k \text{ such that } \\ \dplus(x) = \dminus(x) = 1 \end{subarray}} \frac{1}{\abu(x)}\mathbb{I}(x,a).\]

We consider the proportion
\begin{align*}
r_{k,a} & := \frac{|\ST|}{|\UN \cup \ST|} = \\ 
& = \frac{\sum_{x \in \K_k} \max\left(\frac{1}{\abu(x)}\mathbb{I}(x,a),\frac{1}{\abu(x)}\mathbb{I}(x,a)\dplus(x)\right)}{\sum_{\begin{subarray}{c} x \in \K_k \text{ such that } \\ \dplus(x) = \dminus(x) = 1 \end{subarray}} \frac{1}{\abu(x)}\mathbb{I}(x,a) + \sum_{x \in \K_k} \max\left(\frac{1}{\abu(x)}\mathbb{I}(x,a),\frac{1}{\abu(x)}\mathbb{I}(x,a)\dplus(x)\right)}\in [0,1].\\
\end{align*}

We obtain an estimate $\hat{r}_{k,a}$ of $r_{k,a}$ by sampling a multiset $\{x_1,\dots,x_m\}$ of $k$-mers in $\K_k$ with abundance at least $a$ (that is, for which the indicator variable $\mathbb{I}(x_i,a)$ is 1):
\[\hat{r}_{k,a} := \frac{\sum_{i=1}^{m} \max\left(\frac{1}{\abu(x_i)},\frac{1}{\abu(x_i)}\dplus(x_i)\right)}{\sum_{\begin{subarray}{c} i \in [1,m] \text{ such that } \\ \dplus(x_i) = \dminus(x_i) = 1 \end{subarray}} \frac{1}{\abu(x_i)} + \sum_{i=1}^{m} \max\left(\frac{1}{\abu(x_i)},\frac{1}{\abu(x_i)}\dplus(x_i)\right)}.\]
An estimate $\hat{Z}$ for the quantity from (\ref{eqn:avg_internal}) is then obtained as $1/\hat{r}_{k,a}$. By the observations in Sec.~\ref{sec:sampling-accuracy}, we immediately get how many samples $m$ we need in order to bound the relative error on $\hat{Z}$ within a certain confidence interval.
\alex{say we can implement this for all abundances.}

%\subparagraph{Estimating the average number of nodes in a DBG} % (fold)
%\label{subp:estimating_the_average_number_of_nodes_in_a_dbg}
%Note that we get the number of unitigs in a graph by counting all the start vertices and their outdegree. Let $X_s$ be the number of start nodes in $\mathcal{G}$. We also label all ot in the DBG is given We now divide the set $X$ into $Y$ be the number of internal vertices in the DBG. An internal node in a node that... 

%\subparagraph{Estimating the E-size} % (fold)
%\label{subp:estimating_the_e_size}

\medskip
\noindent\textbf{Estimating the E-size of the unitigs of a dBG.} The E-size of the set $\U$ of unitig strings of $\DB$ is defined as the expected length of the unitig strings of $\DB$ \cite{??}. Formally, 
\[\esize(\U) := \sum_{w \in \U}|w|\mathbb{P}(w) = \sum_{w \in \U} |w|\frac{|w|}{\sum_{w' \in \U}|w'|} = \frac{\sum_{w \in \U}|w|^2}{\sum_{w \in \U}|w|},\]
where $|w|$ denotes the length of the string spelled by the unitig $w$. We also denote the number of nodes of a unitig $w$ as $||w||$. In order to derive an estimate for the E-size, we will construct a sampling procedure which may sample a unitig more than once. Since the above formula contains each unitig once, the main issue here is that we need to normalize it \alex{with the expected number of times of sampling each unitig. [IS THIS STILL CORRECT?]}

Our sampling procedure produces a multiset $W$ of unitigs of $\DB$, as follows: we choose a $k$-mer $x \in \K_k$ at random, and if $x$ is a node of $\DB$, we output all the unitigs containing $x$. These unitigs can be obtained by traversing the graph along the in-/out-neighbors of $x$, after taking into account whether $x$ is a unary node of $\DB$ or not. 

Suppose that the above procedure samples every node of $\DB$ exactly once, and that $\overline{W}$ is the resulting multiset of unitigs. Then, each unitig $w := (v_1,v_2,\dots,v_t)$ of $\DB$ appears $\alpha(w) := \sum_{i = 1}^{t}\alpha(v_i)$ times in $\overline{W}$. However, since not all abundances are equal, we need to remove the bias from over- or under-sampling $k$-mers due to their different abundances. Observe that if all $k$-mers of $\DB$ had the same abundance $a$, then $w$ would appear $ta$ times in $\overline{W}$. Therefore, we can equivalently express the E-size of the set $\U$ by normalizing the probability of $w$ with $a||w||/\abu(w)$. This gives the following expression:
\begin{equation}
\esize(\U) = \sum_{w \in \overline{W}} |w| \frac{|w|\frac{a||w||}{\alpha(w)}}{\sum_{w \in \overline{W}}|w|\frac{a||w||}{\alpha(w)}} = \sum_{w \in \overline{W}} |w| \frac{|w|\frac{||w||}{\alpha(w)}}{\sum_{w \in \overline{W}}|w|\frac{||w||}{\alpha(w)}}.
\label{eqn:e-size}
\end{equation}

However, we cannot afford to sample all $k$-mers in $\K_k$. If sampling only a subset of $k$-mers we obtain the multiset $W = \{w_1,\dots,w_m\}$ of unitigs, the above relation (\ref{eqn:e-size}) shows that we can estimate $\esize(\U)$ as 
\[\hat{{\sf E}}_{\sf size} := \sum_{i = 1}^{m} |w_i| \frac{|w_i|\frac{||w_i||}{\alpha(w)}}{\sum_{j=1}^{m}|w_j|\frac{||w_j||}{\alpha(w_j)}}.\]

\alex{Say that here we cannot guarantee a sampling accuracy (if so).}

%Consider the following procedure which produces a multiset containing the set $\U$ of all unitigs of $\DB$. For all $k$-mers $x$ in the multiset $\K_k$ do:
%\begin{itemize}
%\item if $x$ is a unary node of $\DB$, that is $|N^-(x)| = |N^+(x)| = 1$, then we traverse $\DB$ starting from $x$ and following out-neighbors, until seeing a non-unary node. Then, we traverse $\DB$ starting from $x$ backwards by following in-neighbors, until seeing a non-unary node. The concatenation of these paths is the unitig we report for $x$.
%\item if $x$ is a node of $\DB$ but it is not unary, then for any out-neighbor of $x$ we traverse $\DB$ following out-neighbors, until seeing the first non-unary node, and report each such unitig for $x$. Similarly for every in-neighbor of $x$.
%\end{itemize}


% subparagraph estimating_the_average_number_of_nodes_in_a_dbg (end)


% subsection algorithm (end)

\subsection{Sampling accuracy\label{sec:sampling-accuracy}}

Suppose that we have a set partitioned as $A \cup B$, and we need to estimate the proportion $p = |A| / (|A| + |B|) \in [0,1]$. Suppose that we sample $m$ elements of $A \cup B$ and for each of them record whether they belong to $A$ or to $B$, and then divide these two counts by $m$, obtaining in this way an estimate $\hat{p}$ of $p$. It is a standard result that the $100(1-\alpha)\%$ confidence interval of $\hat{p}$ is
\[\left[\hat{p} - z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{m}} , \hat{p} + z_{\frac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{m}}\right] \]
where $z_{\frac{\alpha}{2}}$ is the $\alpha/2$ quantile from the normal distribution. For a given relative error $\varepsilon$, we want to choose the sample size $n$ such that the $100(1-\alpha)\%$ confidence interval of $\hat{p}$ has a margin of error no more than $E := \varepsilon p$. By standard means, we obtain
\begin{equation}
	m \geq \left(\frac{z_{\frac{\alpha}{2}}}{E}\right)^2\hat{p}(1-\hat{p}).
	\label{eqn:sample-size}
\end{equation}
Notice that in the relation (\ref{eqn:sample-size}) above, both $p$ and $\hat{p}$ are not known at the start of the sampling, when the value of $m$ needs to be chosen. 

\alex{In our case, we choose $p$ and $\hat{p}$ ...  }

Moreover, if we want to estimate $f = 1/p$ with a given relative error $\varepsilon'$, then we can estimate it as $\hat{f} = 1/\hat{p}$. In this case, we need to set the relative error $\varepsilon$ on $\hat{p}$ as $\varepsilon = 1/(1+\varepsilon') - 1$. The above observation about $\hat{p}$ tell how many samples $m$ we need as a relation on $\varepsilon = 1/(1+\varepsilon') - 1$.

%\subparagraph{Sample proportion} % (fold)
%\label{subp:sample_proportion}

% subparagraph sample_proportion (end)

% subparagraph sample_proportion_relative_error (end)

%\subparagraph{Fraction of sample proportions} % (fold)
%\label{subp:fraction_of_proportions} 

%\noindent \textbf{Fraction of sample proportions.} Instead of estimating the proportion $p = |A| / (|A| + |B|)$, we now want to have an estimate of $f =\frac{p}{1-p}$. 
%
%Suppose that we can obtain an estimate $\hat{p}$ of $p$ such that the $100(1-\alpha)\%$ confidence interval of $\hat{p}$ has a margin of error no more than $\varepsilon p$, for any given $\varepsilon$. 
%
%We will estimate $f$ as
%\begin{equation}
% 	\hat{f} = \frac{\hat{p}}{1-\hat{p}}.
%	\label{eqn:sample-fraction}
%\end{equation} 
%For a given relative error $\varepsilon_f$, we now want to choose $\varepsilon$ such that the $100(1-\alpha)\%$ confidence interval of $\hat{f}$ has a margin of error no more than $E_f := \varepsilon_f f$. Plugging $\varepsilon$ and $\varepsilon_f$ into (\ref{eqn:sample-fraction}), we obtain
%
%\[(1+\varepsilon_f)\frac{p}{1-p} = \frac{(1+\varepsilon)p}{1-(1+\varepsilon)p}.\]
%This implies that
%\begin{equation}
%\varepsilon = \frac{\varepsilon_f(1-p)}{1+\varepsilon_f p}.
%\label{eqn:choice-eps-f}
%\end{equation}

% OLD version, it has some bugs
%We have
%\begin{equation*}
% 	(1\pm \varepsilon_f) = \frac{(1\pm \varepsilon_p)(1-p)}{(1\pm \varepsilon_p)p}. 
% \end{equation*} 
%Notice that the margin of error increases as $p$ decreases. Fixing $p$, the error of $f$ is maximized by  
%\begin{equation*}
% 	(1\pm \varepsilon_f) = \frac{(1 + \varepsilon_p)(1-p)}{(1- \varepsilon_p)p}. 
% \end{equation*} 
% Finally, since we sample $p$, we solve this equation for $\varepsilon_p$ and get
%\begin{equation}
% 	\varepsilon_p = \frac{\varepsilon_f}{2 + \varepsilon_f}. 
% \end{equation}

%With the above equation, we now have a way to see what margin of error $\varepsilon_p$ we require to arrive at a fixed margin of error of $f$. That is, if we want to have at most 10\% error of our estimate $\hat{f}$, we need to have a sample size that calculated from letting $\varepsilon = \frac{0.1}{2 + 0.1}$.

%Therefore, tis we estimate $\hat{p}$ as explained in the previous paragraphs, the sample size $n$ for $\hat{p}$ needs to be chosen for the relative error $\varepsilon$ from (\ref{eqn:choice-eps-f}). \alex{Note again that $p$ is an unknown ... }

% subparagraph fraction_of_proportions (end)

% subsubsection theory (end)



%\subsubsection{Application to sampling DBGs} % (fold)
%\label{sub:application_to_sampling_db_graphs}
%We will use the theory in~\ref{ssub:theory} to get accurate sample estimates of the desired quantities. 

% subsection application_to_sampling_db_graphs (end)

% section methods (end)

\section{Results and discussion} % (fold)
\label{sec:results_and_discussion}

% section results_and_discussion (end)

\section{Conclusions} % (fold)
\label{sec:conclusions}

% section conclusions (end)
\end{document}
